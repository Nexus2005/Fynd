# Task 1: Yelp Rating Prediction with LLM Prompting

# This notebook classifies Yelp reviews (1‚Äì5 stars) using multiple prompting strategies.

import os
import json
import time
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Any
from datetime import datetime
import google.generativeai as genai

# ----- CONFIG -----
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
MODEL_NAME = "models/gemini-2.5-flash"
SAMPLE_SIZE = 200
MAX_RETRIES = 3
RETRY_DELAY = 1.5

if not GEMINI_API_KEY:
    raise ValueError("‚ùå GEMINI_API_KEY not found. Set it before running.")

genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel(MODEL_NAME)

print("‚úì Gemini initialized")
print("Using model:", MODEL_NAME)



# Load Yelp dataset (CSV already uploaded in ChatGPT)
csv_path = "/mnt/data/yelp.csv"

df = pd.read_csv(csv_path)
print("Columns:", df.columns.tolist())

# Try auto-detecting columns
rating_col = None
text_col = None

for col in df.columns:
    lc = col.lower()
    if "star" in lc or "rating" in lc:
        rating_col = col
    if "text" in lc or "review" in lc:
        text_col = col

if rating_col is None or text_col is None:
    raise ValueError("‚ùå Unable to detect rating/review columns.")

df = df[[text_col, rating_col]].rename(columns={text_col: "text", rating_col: "stars"})

# Keep only valid 1-5 ratings
df = df[df["stars"].isin([1,2,3,4,5])]

# Sample 200 rows
df = df.sample(n=min(SAMPLE_SIZE, len(df)), random_state=42).reset_index(drop=True)

print("‚úì Loaded dataset")
print(df.head())
print(df["stars"].value_counts())


ZERO_SHOT_PROMPT = """
You classify Yelp reviews into star ratings (1‚Äì5).

Review:
{review_text}

Return JSON:
{
  "predicted_stars": <1-5>,
  "explanation": "<reason>"
}
"""

FEW_SHOT_PROMPT = """
You classify Yelp reviews into star ratings (1‚Äì5).

Examples:
Review: "Amazing food and friendly staff."
Response: {"predicted_stars": 5, "explanation": "Strong positive sentiment"}

Review: "Food was okay, nothing special."
Response: {"predicted_stars": 3, "explanation": "Neutral/average"}

Review: "Terrible experience. Not coming back."
Response: {"predicted_stars": 1, "explanation": "Strongly negative"}

Now classify:
Review: {review_text}

Return JSON:
{
  "predicted_stars": <1-5>,
  "explanation": "<reason>"
}
"""

COT_PROMPT = """
You classify Yelp reviews into star ratings (1‚Äì5).
Think step-by-step about sentiment, tone, complaints, praise, and overall satisfaction.
Then output JSON ONLY.

Review:
{review_text}

Return:
{
  "predicted_stars": <1-5>,
  "explanation": "<detailed chain-of-thought summary>"
}
"""

print("‚úì Prompt templates ready.")



import re

def safe_json_parse(text: str) -> Dict[str, Any]:
    try:
        match = re.search(r"\{.*\}", text, re.DOTALL)
        if match:
            return json.loads(match.group())

        # fallback: extract a digit from text
        d = re.search(r"\b([1-5])\b", text)
        stars = int(d.group(1)) if d else 3

        return {"predicted_stars": stars, "explanation": "Fallback JSON parser"}

    except Exception:
        return {"predicted_stars": 3, "explanation": "Parsing error"}

def call_gemini(prompt: str):
    for attempt in range(MAX_RETRIES):
        try:
            resp = model.generate_content(prompt)
            if resp.text:
                return safe_json_parse(resp.text)
        except Exception as e:
            print(f"API fail (attempt {attempt+1}):", e)
            time.sleep(RETRY_DELAY)
    return {"predicted_stars": 3, "explanation": "API failed"}


    def accuracy(pred, actual):
    return np.mean([p == a for p, a in zip(pred, actual)])

def json_valid(results):
    return np.mean([
        ("predicted_stars" in r and "explanation" in r)
        for r in results
    ])

def consistency(results):
    preds = [r.get("predicted_stars", 3) for r in results]
    return 1 / (1 + np.std(preds))

print("‚úì Evaluation functions ready.")


def run_experiment(df, template, name):
    print(f"\nüöÄ Running experiment: {name}")
    results = []
    preds = []

    for i, row in df.iterrows():
        prompt = template.format(review_text=row["text"][:350])
        r = call_gemini(prompt)
        results.append(r)
        preds.append(r["predicted_stars"])

        if i % 20 == 0:
            print(f"{i}/{len(df)} processed...")

    return {
        "name": name,
        "results": results,
        "preds": preds,
        "acc": accuracy(preds, df["stars"]),
        "json_ok": json_valid(results),
        "consistency": consistency(results),
    }

exp_zero = run_experiment(df, ZERO_SHOT_PROMPT, "Zero-Shot")
exp_few = run_experiment(df, FEW_SHOT_PROMPT, "Few-Shot")
exp_cot = run_experiment(df, COT_PROMPT, "Chain-of-Thought")

experiments = [exp_zero, exp_few, exp_cot]

print("\n‚úì All experiments complete.")


summary = pd.DataFrame([
    {
        "Prompt": exp["name"],
        "Accuracy": exp["acc"],
        "JSON Validity": exp["json_ok"],
        "Consistency": exp["consistency"],
    }
    for exp in experiments
])

summary


plt.figure(figsize=(10,6))
plt.bar(summary["Prompt"], summary["Accuracy"])
plt.ylim(0,1)
plt.title("Accuracy Comparison")
plt.show()

plt.figure(figsize=(10,6))
plt.bar(summary["Prompt"], summary["JSON Validity"])
plt.ylim(0,1)
plt.title("JSON Validity Comparison")
plt.show()

plt.figure(figsize=(10,6))
plt.bar(summary["Prompt"], summary["Consistency"])
plt.ylim(0,1)
plt.title("Consistency Comparison")
plt.show()


output_dir = "/mnt/data/yelp_output"
os.makedirs(output_dir, exist_ok=True)

summary.to_csv(f"{output_dir}/summary.csv", index=False)

print("‚úì Saved summary to /mnt/data/yelp_output/summary.csv")
print("üéâ TASK 1 COMPLETED SUCCESSFULLY!")
